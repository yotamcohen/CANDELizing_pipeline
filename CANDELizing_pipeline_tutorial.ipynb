{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports for this notebook\n",
    "from pandas import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion()\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# yotam's CANDELizing pipeline for SAM mock catalog\n",
    "\n",
    "In this notebook, we demonstrate how to take a lightcone catalog file generated by the Somerville+ SAM and run it through our \"CANDELizing\" pipeline. The pipeline has several goals including correcting the mock catalog for CANDELS completeness limitations, correcting galaxy sizes, and adding photometric noise.\n",
    "\n",
    "Dependencies:\n",
    "* python (2.7+ probably best)\n",
    "* NumPy (http://www.numpy.org/)\n",
    "* Pandas (http://pandas.pydata.org/)\n",
    "* PyTables (http://www.pytables.org/)\n",
    "* H5pi (http://www.h5py.org/)\n",
    "* CosmoloPy (http://roban.github.io/CosmoloPy/)\n",
    "\n",
    "\n",
    "You should also upgrade all those packages (except python, best to use 2.7) to their latest versions to avoid incompatibility issues. This can be accomplishes with pip:\n",
    "\n",
    "    pip install --upgrade numpy\n",
    "    pip install --upgrade pandas\n",
    "    pip install --upgrade tables\n",
    "    pip install --upgrade h5py\n",
    "    pip install --upgrade cosmolopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download\n",
    "\n",
    "First, download the pipeline repository, if you haven't already, and move into that directory. It can be found at:\n",
    "\n",
    "https://github.com/yotamcohen/CANDELizing_pipeline\n",
    "\n",
    "Lets see whats inside:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CANDELizing_pipeline_tutorial.ipynb\n",
      "example_lightcone.dat\n",
      "example_lightcone.h5\n",
      "lib/\n",
      "plots/\n",
      "python_scripts/\n",
      "readme.txt\n",
      "runpipeline.sh*\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls -F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so we see directories containing library files and python scripts, and an example lightcone file called example_lightcone.dat, which we will use in this demonstration. This example lightcone file is a subset of a full lightcone.dat file from a GOODS-S realization, trimmed down to a managable size for example purposes. You can also use it for testing the pipeline on your own machine. The lightcone file is expected to be in the particular format as the example one we have here, complete with the header rows and all of the columns output by the SAM code. The version of the SAM code used for this paper writes 142 columns to the lightcone.dat file by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 0 halo_id_nbody\n",
      "# 1 gal_id\n",
      "# 2 gal_type (0, 1, 2)\n",
      "# 3 z_nopec [] (distance redshift w/o pec velocity)\n",
      "# 4 redshift []\n",
      "# 5 ra [degree]\n",
      "# 6 dec [degree]\n",
      "# 7 m_vir [1.0E10 M_sun]\n",
      "# 8 V_vir [km/s]\n",
      "# 9 r_vir [Mpc]\n",
      "# 10 c_NFW []\n",
      "# 11 spin []\n",
      "# 12 mstar_diffuse [1.0E10 M_sun]\n",
      "# 13 m_hot_halo [1.0E10 M_sun]\n",
      "# 14 Z_hot_halo [Z_sun]\n",
      "# 15 v_disk [km/s]\n",
      "# 16 r_disk [kpc]\n",
      "# 17 sigma_bulge [km/s]\n",
      "# 18 rbulge [kpc]\n",
      "# 19 mhalo [1.0E10 M_sun]\n",
      "# 20 mstar [1.0E10 M_sun]\n",
      "# 21 mcold [1.0E10 M_sun]\n",
      "# 22 mbulge [1.0E10 M_sun]\n",
      "# 23 mbh [1.0E10 M_sun]\n",
      "# 24 maccdot [M_sun/yr]\n",
      "# 25 maccdot_radio [M_sun/yr]\n",
      "# 26 Zstar [Z_sun]\n",
      "# 27 Zcold [Z_sun]\n",
      "# 28 mstardot [M_sun/yr]\n",
      "# 29 sfr_ave d [M_sun/yr]\n",
      "# 30 meanage [Gyr]\n",
      "# 31 tmerge [Gyr]\n",
      "# 32 tmajmerge [Gyr]\n",
      "# 33 cosi []\n",
      "# 34 UV1500_rest CANDELS/TOPHAT/UV1500top.dat\n",
      "# 35 UV1500_rest_bulge CANDELS/TOPHAT/UV1500top.dat\n",
      "# 36 UV1500_rest_dust CANDELS/TOPHAT/UV1500top.dat\n",
      "# 37 UV2300_rest CANDELS/TOPHAT/UV2300top.dat\n",
      "# 38 UV2300_rest_bulge CANDELS/TOPHAT/UV2300top.dat\n",
      "# 39 UV2300_rest_dust CANDELS/TOPHAT/UV2300top.dat\n",
      "# 40 UV2800_rest CANDELS/TOPHAT/UV2800top.dat\n",
      "# 41 UV2800_rest_bulge CANDELS/TOPHAT/UV2800top.dat\n",
      "# 42 UV2800_rest_dust CANDELS/TOPHAT/UV2800top.dat\n",
      "# 43 U_rest CANDELS/Johnson/Johnson_U.dat\n",
      "# 44 U_rest_bulge CANDELS/Johnson/Johnson_U.dat\n",
      "# 45 U_rest_dust CANDELS/Johnson/Johnson_U.dat\n",
      "# 46 B_rest CANDELS/Johnson/Johnson_B.dat\n",
      "# 47 B_rest_bulge CANDELS/Johnson/Johnson_B.dat\n",
      "# 48 B_rest_dust CANDELS/Johnson/Johnson_B.dat\n",
      "# 49 V_rest CANDELS/Johnson/Johnson_V.dat\n",
      "# 50 V_rest_bulge CANDELS/Johnson/Johnson_V.dat\n",
      "# 51 V_rest_dust CANDELS/Johnson/Johnson_V.dat\n",
      "# 52 R_rest CANDELS/Johnson/Cousins_R.dat\n",
      "# 53 R_rest_bulge CANDELS/Johnson/Cousins_R.dat\n",
      "# 54 R_rest_dust CANDELS/Johnson/Cousins_R.dat\n",
      "# 55 I_rest CANDELS/Johnson/Cousins_I.dat\n",
      "# 56 I_rest_bulge CANDELS/Johnson/Cousins_I.dat\n",
      "# 57 I_rest_dust CANDELS/Johnson/Cousins_I.dat\n",
      "# 58 J_rest CANDELS/Johnson/Bessell_J.dat\n",
      "# 59 J_rest_bulge CANDELS/Johnson/Bessell_J.dat\n",
      "# 60 J_rest_dust CANDELS/Johnson/Bessell_J.dat\n",
      "# 61 H_rest CANDELS/Johnson/Bessell_H.dat\n",
      "# 62 H_rest_bulge CANDELS/Johnson/Bessell_H.dat\n",
      "# 63 H_rest_dust CANDELS/Johnson/Bessell_H.dat\n",
      "# 64 K_rest CANDELS/Johnson/Bessell_K.dat\n",
      "# 65 K_rest_bulge CANDELS/Johnson/Bessell_K.dat\n",
      "# 66 K_rest_dust CANDELS/Johnson/Bessell_K.dat\n",
      "# 67 galex_FUV CANDELS/GALEX/galex_FUV.dat\n",
      "# 68 galex_FUV_bulge CANDELS/GALEX/galex_FUV.dat\n",
      "# 69 galex_FUV_dust CANDELS/GALEX/galex_FUV.dat\n",
      "# 70 galex_NUV CANDELS/GALEX/galex_NUV.dat\n",
      "# 71 galex_NUV_bulge CANDELS/GALEX/galex_NUV.dat\n",
      "# 72 galex_NUV_dust CANDELS/GALEX/galex_NUV.dat\n",
      "# 73 sdss_u CANDELS/SDSS/sdss_u.dat\n",
      "# 74 sdss_u_bulge CANDELS/SDSS/sdss_u.dat\n",
      "# 75 sdss_u_dust CANDELS/SDSS/sdss_u.dat\n",
      "# 76 sdss_g CANDELS/SDSS/sdss_g.dat\n",
      "# 77 sdss_g_bulge CANDELS/SDSS/sdss_g.dat\n",
      "# 78 sdss_g_dust CANDELS/SDSS/sdss_g.dat\n",
      "# 79 sdss_r CANDELS/SDSS/sdss_r.dat\n",
      "# 80 sdss_r_bulge CANDELS/SDSS/sdss_r.dat\n",
      "# 81 sdss_r_dust CANDELS/SDSS/sdss_r.dat\n",
      "# 82 sdss_i CANDELS/SDSS/sdss_i.dat\n",
      "# 83 sdss_i_bulge CANDELS/SDSS/sdss_i.dat\n",
      "# 84 sdss_i_dust CANDELS/SDSS/sdss_i.dat\n",
      "# 85 sdss_z CANDELS/SDSS/sdss_z.dat\n",
      "# 86 sdss_z_bulge CANDELS/SDSS/sdss_z.dat\n",
      "# 87 sdss_z_dust CANDELS/SDSS/sdss_z.dat\n",
      "# 88 acsf435w CANDELS/ACS/f435w.WFC1.dat\n",
      "# 89 acsf435w_bulge CANDELS/ACS/f435w.WFC1.dat\n",
      "# 90 acsf435w_dust CANDELS/ACS/f435w.WFC1.dat\n",
      "# 91 acsf606w CANDELS/ACS/f606w.WFC1.dat\n",
      "# 92 acsf606w_bulge CANDELS/ACS/f606w.WFC1.dat\n",
      "# 93 acsf606w_dust CANDELS/ACS/f606w.WFC1.dat\n",
      "# 94 acsf775w CANDELS/ACS/f775w.WFC1.dat\n",
      "# 95 acsf775w_bulge CANDELS/ACS/f775w.WFC1.dat\n",
      "# 96 acsf775w_dust CANDELS/ACS/f775w.WFC1.dat\n",
      "# 97 acsf814w CANDELS/ACS/f814w.WFC1.dat\n",
      "# 98 acsf814w_bulge CANDELS/ACS/f814w.WFC1.dat\n",
      "# 99 acsf814w_dust CANDELS/ACS/f814w.WFC1.dat\n",
      "# 100 acsf850lp CANDELS/ACS/f850lp.WFC1.dat\n",
      "# 101 acsf850lp_bulge CANDELS/ACS/f850lp.WFC1.dat\n",
      "# 102 acsf850lp_dust CANDELS/ACS/f850lp.WFC1.dat\n",
      "# 103 wfc3f275w CANDELS/WFC3/f275w.UVIS1.dat\n",
      "# 104 wfc3f275w_bulge CANDELS/WFC3/f275w.UVIS1.dat\n",
      "# 105 wfc3f275w_dust CANDELS/WFC3/f275w.UVIS1.dat\n",
      "# 106 wfc3f336w CANDELS/WFC3/f336w.UVIS1.dat\n",
      "# 107 wfc3f336w_bulge CANDELS/WFC3/f336w.UVIS1.dat\n",
      "# 108 wfc3f336w_dust CANDELS/WFC3/f336w.UVIS1.dat\n",
      "# 109 wfc3f105w CANDELS/WFC3/f105w.IR.dat\n",
      "# 110 wfc3f105w_bulge CANDELS/WFC3/f105w.IR.dat\n",
      "# 111 wfc3f105w_dust CANDELS/WFC3/f105w.IR.dat\n",
      "# 112 wfc3f125w CANDELS/WFC3/f125w.IR.dat\n",
      "# 113 wfc3f125w_bulge CANDELS/WFC3/f125w.IR.dat\n",
      "# 114 wfc3f125w_dust CANDELS/WFC3/f125w.IR.dat\n",
      "# 115 wfc3f160w CANDELS/WFC3/f160w.IR.dat\n",
      "# 116 wfc3f160w_bulge CANDELS/WFC3/f160w.IR.dat\n",
      "# 117 wfc3f160w_dust CANDELS/WFC3/f160w.IR.dat\n",
      "# 118 ctio_U CANDELS/CTIO/U_ctio_mosaic_tot.dat\n",
      "# 119 ctio_U_bulge CANDELS/CTIO/U_ctio_mosaic_tot.dat\n",
      "# 120 ctio_U_dust CANDELS/CTIO/U_ctio_mosaic_tot.dat\n",
      "# 121 CFHTLS_u CANDELS/CFHTLS/uMega.fil\n",
      "# 122 CFHTLS_u_bulge CANDELS/CFHTLS/uMega.fil\n",
      "# 123 CFHTLS_u_dust CANDELS/CFHTLS/uMega.fil\n",
      "# 124 musyc_u38 CANDELS/MUSYC/ecdfs.U38.filt.dat\n",
      "# 125 musyc_u38_bulge CANDELS/MUSYC/ecdfs.U38.filt.dat\n",
      "# 126 musyc_u38_dust CANDELS/MUSYC/ecdfs.U38.filt.dat\n",
      "# 127 UKIRT_J CANDELS/UKIRT/J_filter.dat\n",
      "# 128 UKIRT_J_bulge CANDELS/UKIRT/J_filter.dat\n",
      "# 129 UKIRT_J_dust CANDELS/UKIRT/J_filter.dat\n",
      "# 130 UKIRT_H CANDELS/UKIRT/H_filter.dat\n",
      "# 131 UKIRT_H_bulge CANDELS/UKIRT/H_filter.dat\n",
      "# 132 UKIRT_H_dust CANDELS/UKIRT/H_filter.dat\n",
      "# 133 UKIRT_K CANDELS/UKIRT/K_filter.dat\n",
      "# 134 UKIRT_K_bulge CANDELS/UKIRT/K_filter.dat\n",
      "# 135 UKIRT_K_dust CANDELS/UKIRT/K_filter.dat\n",
      "# 136 irac_ch1 CANDELS/IRAC/irac_ch1.dat\n",
      "# 137 irac_ch1_bulge CANDELS/IRAC/irac_ch1.dat\n",
      "# 138 irac_ch1_dust CANDELS/IRAC/irac_ch1.dat\n",
      "# 139 irac_ch2 CANDELS/IRAC/irac_ch2.dat\n",
      "# 140 irac_ch2_bulge CANDELS/IRAC/irac_ch2.dat\n",
      "# 141 irac_ch2_dust CANDELS/IRAC/irac_ch2.dat\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head -n 142 example_lightcone.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wrapper\n",
    "\n",
    "There is a wrapper included called `runpipeline.sh`. If you want to attempt to run the whole pipeline at once, just go inside this file, set the path to the working directory (which should be the directory containing the pipeline), the path to the original lightcone file, and the path to the (soon to be created) h5file. Then, just run the wrapper. To see how the individual steps work, continue reading on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grabbed lightcone of shape (12158, 142)\n",
      "writing to h5 file...\n",
      "saved lightcone data to /Users/yotam/CANDELizing_pipeline/example_lightcone.h5\n",
      "all units are same as in original lightcone data file\n",
      "grabbed dataframe. working...\n",
      "saved dataframe to /Users/yotam/CANDELizing_pipeline/example_lightcone.h5\n",
      "grabbed dataframe from /Users/yotam/CANDELizing_pipeline/example_lightcone.h5\n",
      "dataframe now includes bulge/disk ratio from projected sizes\n",
      "dataframe now includes sersic profile parameters\n",
      "dataframe now includes galaxy radii in PIXELS as seen by HSTWFC3F160W\n",
      "saved dataframe to /Users/yotam/CANDELizing_pipeline/example_lightcone.h5\n",
      "grabbed lightcone\n",
      "grabbed photometry from CANDELS\n",
      "acs_f435w_mag (37394, 308)\n",
      "acs_f606w_v08_mag (8208, 308)\n",
      "acs_f775w_mag (40288, 308)\n",
      "acs_f814w_v08_mag (8224, 308)\n",
      "acs_f850lp_mag (40777, 308)\n",
      "wfc3_f275w_mag (4885, 308)\n",
      "wfc3_f105w_mag (33926, 308)\n",
      "wfc3_f125w_v08_mag (8215, 308)\n",
      "wfc3_f160w_v08_mag (8236, 308)\n",
      "irac_ch1_mag (23652, 308)\n",
      "irac_ch2_mag (23629, 308)\n",
      "dataframe now includes photometric noise\n",
      "saved to /Users/yotam/CANDELizing_pipeline/example_lightcone.h5\n",
      "working on /Users/yotam/CANDELizing_pipeline/example_lightcone.h5\n",
      "dataframe now includes detection probabilities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python_scripts/photometric_noise_new.py:82: RuntimeWarning: invalid value encountered in log10\n",
      "  mdf[mmagnoisy] = -2.5*np.log10(mdf[mfluxnoisy]) + 23.9\n",
      "python_scripts/photometric_noise_new.py:85: RuntimeWarning: invalid value encountered in log10\n",
      "  mdf[mmagdustnoisy] = -2.5*np.log10(mdf[mfluxdustnoisy]) + 23.9\n",
      "python_scripts/calculate_dp.py:32: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  Hd[np.isneginf(1/Hd)] = np.nan\n",
      "python_scripts/calculate_dp.py:33: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  He[np.isneginf(1/He)] = np.nan\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./runpipeline.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: ascii text file to hdf5 file\n",
    "\n",
    "The native format of the lightcone.dat files output by the SAM is simple ASCII text, which can be very slow to read/write with python, so the first step is to store all the data in an hdf5 file. For this, we will use python_scripts/rawlightconefile_to_h5.py. This script is expecting a header file containing the native header rows, which should live in the lib/ directory; if it doesn't already exist, you can just do something like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "head -n 142 example_lightcone.dat > lib/headerfile.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the h5 conversion script from the command line (from this directory), just do:\n",
    "\n",
    "    python python_scripts/rawlightconefile_to_h5.py <headerfile> <lightcone file> <desired path to h5file>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grabbed lightcone of shape (12158, 142)\n",
      "writing to h5 file...\n",
      "saved lightcone data to ./example_lightcone.h5\n",
      "all units are same as in original lightcone data file\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python python_scripts/rawlightconefile_to_h5.py lib/headerfile.txt ./example_lightcone.dat ./example_lightcone.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 115008\n",
      "-rw-r--r--   1 yotam  staff    16M Jan 22 15:54 example_lightcone.dat\n",
      "-rw-r--r--   1 yotam  staff   121B Jan 22 15:54 readme.txt\n",
      "drwxr-xr-x  13 yotam  staff   442B Jan 22 15:54 python_scripts/\n",
      "drwxr-xr-x   9 yotam  staff   306B Jan 22 15:54 plots/\n",
      "drwxr-xr-x  12 yotam  staff   408B Jan 22 15:54 lib/\n",
      "-rwxr-xr-x   1 yotam  staff   535B Jan 24 13:50 runpipeline.sh*\n",
      "-rw-r--r--   1 yotam  staff    32K Jan 24 13:51 CANDELizing_pipeline_tutorial.ipynb\n",
      "-rw-r--r--   1 yotam  staff    40M Jan 24 13:51 example_lightcone.h5\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls -ltrhF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so we see the file was created. Lets open it up to make sure everythings there. You can easily open it with pandas in an interactive python sessios (like this notebook). By default, the data will be stored under the 'dat' key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   halo_id_nbody  gal_id  gal_type   z_nopec  redshift         ra        dec  \\\n",
      "0     2776575133       1         0  0.018547  0.020271  53.374077 -27.527531   \n",
      "1     2691101762       1         0  0.074530  0.076570  52.987942 -27.819973   \n",
      "2     2677376520      12         2  0.075058  0.077036  53.005903 -27.613504   \n",
      "3     2661838289      16         2  0.085732  0.083417  53.351725 -27.662076   \n",
      "4     2661851549       1         0  0.090251  0.089146  52.952389 -27.695505   \n",
      "\n",
      "       m_vir      V_vir     r_vir      ...        UKIRT_H_dust    UKIRT_K  \\\n",
      "0   2.423304  37.088756  0.075312      ...           20.788730  21.144127   \n",
      "1   1.485251  32.040132  0.061878      ...           25.996959  26.278141   \n",
      "2  14.719764  68.831694  0.132876      ...           21.741104  21.965658   \n",
      "3  10.148968  60.924214  0.116946      ...           23.278740  23.523828   \n",
      "4   1.074189  28.868147  0.055132      ...           26.129287  26.398970   \n",
      "\n",
      "   UKIRT_K_bulge  UKIRT_K_dust   irac_ch1  irac_ch1_bulge  irac_ch1_dust  \\\n",
      "0      24.537762     21.144127  21.939049       25.337122      21.939049   \n",
      "1      29.281053     26.278141  27.071520       30.078001      27.071520   \n",
      "2      23.674726     21.970118  22.752666       24.458108      22.752746   \n",
      "3      27.004021     23.523828  24.317908       27.795882      24.317908   \n",
      "4      29.779144     26.398970  27.193208       30.574421      27.193208   \n",
      "\n",
      "    irac_ch2  irac_ch2_bulge  irac_ch2_dust  \n",
      "0  22.395286       25.802917      22.395286  \n",
      "1  27.457822       30.507695      27.457822  \n",
      "2  23.196188       24.908618      23.196188  \n",
      "3  24.729002       28.237642      24.729002  \n",
      "4  27.582015       31.002708      27.582015  \n",
      "\n",
      "[5 rows x 142 columns]\n"
     ]
    }
   ],
   "source": [
    "store = HDFStore('example_lightcone.h5')\n",
    "lcone = store['dat']\n",
    "store.close()\n",
    "\n",
    "print lcone.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there's our lightcone data, now stored in h5 format for efficient reading/writing for the rest of the pipeline and just accessing the data in general for e.g. plotting, analysis, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: galaxy size corrections\n",
    "\n",
    "The SAM code reports the 3D bulge and disk sizes. We want to also have the 2D (projected) sizes. There is a perscription for how to do this conversion, which is implemented in python_scripts/galaxy_size_corrections.py. The usage is simple, just call the script with the h5file from the previous step as the only argument:\n",
    "\n",
    "    python python_scripts/galaxy_size_corrections.py <lightcone h5 file>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grabbed dataframe. working...\n",
      "saved dataframe to example_lightcone.h5\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python python_scripts/galaxy_size_corrections.py example_lightcone.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   halo_id_nbody  gal_id  gal_type   z_nopec  redshift         ra        dec  \\\n",
      "0     2776575133       1         0  0.018547  0.020271  53.374077 -27.527531   \n",
      "1     2691101762       1         0  0.074530  0.076570  52.987942 -27.819973   \n",
      "2     2677376520      12         2  0.075058  0.077036  53.005903 -27.613504   \n",
      "3     2661838289      16         2  0.085732  0.083417  53.351725 -27.662076   \n",
      "4     2661851549       1         0  0.090251  0.089146  52.952389 -27.695505   \n",
      "\n",
      "       m_vir      V_vir     r_vir    ...     irac_ch1_dust   irac_ch2  \\\n",
      "0   2.423304  37.088756  0.075312    ...         21.939049  22.395286   \n",
      "1   1.485251  32.040132  0.061878    ...         27.071520  27.457822   \n",
      "2  14.719764  68.831694  0.132876    ...         22.752746  23.196188   \n",
      "3  10.148968  60.924214  0.116946    ...         24.317908  24.729002   \n",
      "4   1.074189  28.868147  0.055132    ...         27.193208  27.582015   \n",
      "\n",
      "   irac_ch2_bulge  irac_ch2_dust  r_disk_proj_star  r_bulge_proj_star  \\\n",
      "0       25.802917      22.395286          2.658992           0.002180   \n",
      "1       30.507695      27.457822          3.021128           0.000002   \n",
      "2       24.908618      23.196188          2.024101           0.076702   \n",
      "3       28.237642      24.729002          4.511346           0.003592   \n",
      "4       31.002708      27.582015          2.602500           0.000005   \n",
      "\n",
      "   r_disk_proj_H  r_bulge_proj_H  BT_light   BT_mass  \n",
      "0       4.496740        0.002292  0.044357  0.062245  \n",
      "1       6.502825        0.000002  0.062672  0.120755  \n",
      "2       2.880896        0.081765  0.208211  0.244035  \n",
      "3       7.356324        0.003835  0.041531  0.072405  \n",
      "4       5.457131        0.000006  0.043981  0.087022  \n",
      "\n",
      "[5 rows x 148 columns]\n"
     ]
    }
   ],
   "source": [
    "store = HDFStore('example_lightcone.h5')\n",
    "lcone = store['dat']\n",
    "store.close()\n",
    "\n",
    "print lcone.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are a few new columns at the end of the dataframe, including the projected bulge and disk sizes (in the original units of kpc) in H-band and stellar mass, as well as bulge/total ratios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: various other parameters\n",
    "\n",
    "The main goal of this pipeline is to do a completeness correction on the mock catalogs given the completeness limits of CANDELS. People at STScI have generated lookup tables which give the probability of detecting a galaxy given it's H-band magnitude, effective radius in pixels (given the plate scale of HSTWFC3F160W), and sersic index. In order to use these, we need to calculate and add these parameters to our mock galaxy catalog.\n",
    "\n",
    "* The effective radius and sersic index calculation is done using a model which takes as input the bulge/total ratio (based on light) and bulge size to disk size ratio.\n",
    "* In order to calculate the size in pixels, we first need the angular size. The galaxies are far away, so the small angle approximation tells us that the angular size is just given by the physical size divided by the distance. Since our mock galaxies have large, cosmological redshifts, the distance must be calculated using a cosmology library (this is where CosmoloPy comes in). Once we know the distance, we calculate the angular size, and then we calculate the size in pixels simply by knowning the plate scale of HSTWFC3.\n",
    "\n",
    "The above is implemented in python_scripts/galaxy_profiles.py. Usage is simple:\n",
    "\n",
    "    python python_scripts/galaxy_profiles.py <lighcone h5 file>\n",
    "\n",
    "(Beware that the sersic index calculation is slow and can take several minutes or even more than an hour for a full (>1Gb) lightcone.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grabbed dataframe from example_lightcone.h5\n",
      "dataframe now includes bulge/disk ratio from projected sizes\n",
      "dataframe now includes sersic profile parameters\n",
      "dataframe now includes galaxy radii in PIXELS as seen by HSTWFC3F160W\n",
      "saved dataframe to example_lightcone.h5\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python python_scripts/galaxy_profiles.py example_lightcone.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, lets check the output..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   halo_id_nbody  gal_id  gal_type   z_nopec  redshift         ra        dec  \\\n",
      "0     2776575133       1         0  0.018547  0.020271  53.374077 -27.527531   \n",
      "1     2691101762       1         0  0.074530  0.076570  52.987942 -27.819973   \n",
      "2     2677376520      12         2  0.075058  0.077036  53.005903 -27.613504   \n",
      "3     2661838289      16         2  0.085732  0.083417  53.351725 -27.662076   \n",
      "4     2661851549       1         0  0.090251  0.089146  52.952389 -27.695505   \n",
      "\n",
      "       m_vir      V_vir     r_vir     ...      r_bulge_proj_H  BT_light  \\\n",
      "0   2.423304  37.088756  0.075312     ...            0.002292  0.044357   \n",
      "1   1.485251  32.040132  0.061878     ...            0.000002  0.062672   \n",
      "2  14.719764  68.831694  0.132876     ...            0.081765  0.208211   \n",
      "3  10.148968  60.924214  0.116946     ...            0.003835  0.041531   \n",
      "4   1.074189  28.868147  0.055132     ...            0.000006  0.043981   \n",
      "\n",
      "    BT_mass           RBD  n_sersic      reff            d_a            d_L  \\\n",
      "0  0.062245  5.098135e-04  1.098090  4.331965   87438.757370   91019.629232   \n",
      "1  0.120755  2.799372e-07  1.143273  6.152819  308849.047842  357956.962155   \n",
      "2  0.244035  2.838173e-02  1.700575  2.290047  310559.280446  360250.798019   \n",
      "3  0.072405  5.212547e-04  1.091413  7.106021  333786.931111  391796.561585   \n",
      "4  0.087022  1.008907e-06  1.097198  5.259066  354341.651839  420333.890037   \n",
      "\n",
      "   angular_radius    r_pixels  \n",
      "0       10.218947  170.315782  \n",
      "1        4.109159   68.485990  \n",
      "2        1.520985   25.349754  \n",
      "3        4.391191   73.186513  \n",
      "4        3.061340   51.022331  \n",
      "\n",
      "[5 rows x 155 columns]\n"
     ]
    }
   ],
   "source": [
    "store = HDFStore('example_lightcone.h5')\n",
    "lcone = store['dat']\n",
    "store.close()\n",
    "\n",
    "print lcone.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are several new columns. Just to be clear, lets name them all here:\n",
    "\n",
    "* RBD : ratio of projected bulge radius to projected disk radius\n",
    "* n_sersic : sersic index\n",
    "* reff: projected composite H-band radius in kpc\n",
    "* d_a : cosmological angular diameter distance in kpc\n",
    "* d_L : cosmological luminosity distance in kpc\n",
    "* angular_radius : in arcseconds\n",
    "* r_pixels : given the plate scale of HSTWFC3F160W of 0.06 arcsec/pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: add photometric noise\n",
    "\n",
    "We would now like to add noise to our mock galaxies photometry. One of the main reasons for doing this is because in the following step, we would like to estimate the detection probability at a given observed magnitude. Real observations in the survey are plagued by noise. Thus, we would like to replicate the effect of this noise, and then use the noisy photometry (as opposed to the clean photometry) as the input to our detection probability calculation. The systematics of HST and noise background in various bandpasses is well-understood, so we use this information in order to assign photometric noise to the mock photometry. This is accomplished by running python_scripts/photometric_noise.py. Usage is simple:\n",
    "\n",
    "    python python_scripts/photometric_noise_new.py <lightcone h5 file>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grabbed lightcone\n",
      "grabbed photometry from CANDELS\n",
      "acs_f435w_mag (37394, 308)\n",
      "acs_f606w_v08_mag (8208, 308)\n",
      "acs_f775w_mag (40288, 308)\n",
      "acs_f814w_v08_mag (8224, 308)\n",
      "acs_f850lp_mag (40777, 308)\n",
      "wfc3_f275w_mag (4885, 308)\n",
      "wfc3_f105w_mag (33926, 308)\n",
      "wfc3_f125w_v08_mag (8215, 308)\n",
      "wfc3_f160w_v08_mag (8236, 308)\n",
      "irac_ch1_mag (23652, 308)\n",
      "irac_ch2_mag (23629, 308)\n",
      "dataframe now includes photometric noise\n",
      "saved to example_lightcone.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python_scripts/photometric_noise_new.py:82: RuntimeWarning: invalid value encountered in log10\n",
      "  mdf[mmagnoisy] = -2.5*np.log10(mdf[mfluxnoisy]) + 23.9\n",
      "python_scripts/photometric_noise_new.py:85: RuntimeWarning: invalid value encountered in log10\n",
      "  mdf[mmagdustnoisy] = -2.5*np.log10(mdf[mfluxdustnoisy]) + 23.9\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python python_scripts/photometric_noise_new.py example_lightcone.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^^^ Those warnings are generated because a small fraction of the objects end up with a negative flux after adding the gaussian noise, so the magnitudes cannot be calculated and will appear as a NaN value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: detection probability estimation\n",
    "\n",
    "We now have all the information needed to lookup the probability of detecting a given mock galaxy given the limitations of CANDELS and the properties of the galaxy. This is implemented in python_scripts/calculate_dp.py. Usage is as follows:\n",
    "\n",
    "    python python_scripts/calculate_dp.py <lightcone h5 file> <table for disks> <table for ellipticals>\n",
    "\n",
    "The lookup tables are in lib/completeness_tables. There are tables for each CANDELS field, and they are further split by the galaxy type. All you have to do is call the script with the lookup table files for the CANDELS field corresponding to your lightcone file. In this example, we have been using a subset of a GOODS-S lightcone file, so we will call those lookup tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on example_lightcone.h5\n",
      "dataframe now includes detection probabilities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python_scripts/calculate_dp.py:32: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  Hd[np.isneginf(1/Hd)] = np.nan\n",
      "python_scripts/calculate_dp.py:33: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  He[np.isneginf(1/He)] = np.nan\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python python_scripts/calculate_dp.py example_lightcone.h5 \\\n",
    "./lib/completeness_tables/goodss_expdisk.npz \\\n",
    "./lib/completeness_tables/goodss_devauc.npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check it out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   halo_id_nbody  gal_id  gal_type   z_nopec  redshift         ra        dec  \\\n",
      "0     2776575133       1         0  0.018547  0.020271  53.374077 -27.527531   \n",
      "1     2691101762       1         0  0.074530  0.076570  52.987942 -27.819973   \n",
      "2     2677376520      12         2  0.075058  0.077036  53.005903 -27.613504   \n",
      "3     2661838289      16         2  0.085732  0.083417  53.351725 -27.662076   \n",
      "4     2661851549       1         0  0.090251  0.089146  52.952389 -27.695505   \n",
      "\n",
      "       m_vir      V_vir     r_vir   ...     irac_ch1_dust_mag_noisy  \\\n",
      "0   2.423304  37.088756  0.075312   ...                   21.910410   \n",
      "1   1.485251  32.040132  0.061878   ...                   25.888894   \n",
      "2  14.719764  68.831694  0.132876   ...                   22.792020   \n",
      "3  10.148968  60.924214  0.116946   ...                   24.458672   \n",
      "4   1.074189  28.868147  0.055132   ...                         NaN   \n",
      "\n",
      "   irac_ch2_mag  irac_ch2_flux  irac_ch2_dust_mag  irac_ch2_dust_flux  \\\n",
      "0     22.395286       3.998394          22.395286            3.998394   \n",
      "1     27.457822       0.037746          27.457822            0.037746   \n",
      "2     23.196188       1.912163          23.196188            1.912163   \n",
      "3     24.729002       0.466014          24.729002            0.466014   \n",
      "4     27.582015       0.033666          27.582015            0.033666   \n",
      "\n",
      "   irac_ch2_flux_noisy  irac_ch2_dust_flux_noisy  irac_ch2_mag_noisy  \\\n",
      "0             3.994721                  3.974493           22.396284   \n",
      "1            -0.104071                  0.207172                 NaN   \n",
      "2             1.888784                  1.845292           23.209544   \n",
      "3             0.280991                  0.530567           25.278270   \n",
      "4            -0.054196                 -0.214718                 NaN   \n",
      "\n",
      "   irac_ch2_dust_mag_noisy       dp  \n",
      "0                22.401796  1.00000  \n",
      "1                25.609174  0.00000  \n",
      "2                23.234837  0.64154  \n",
      "3                24.588150  0.00000  \n",
      "4                      NaN  0.00000  \n",
      "\n",
      "[5 rows x 244 columns]\n"
     ]
    }
   ],
   "source": [
    "store = HDFStore('example_lightcone.h5')\n",
    "lcone = store['dat']\n",
    "store.close()\n",
    "\n",
    "print lcone.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There is one new column, called dp (detection probability). Now we can do whatever we want with this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
