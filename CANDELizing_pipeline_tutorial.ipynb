{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports for this notebook\n",
    "from pandas import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion()\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yotam's CANDELizing pipeline for SAM mock catalogs\n",
    "=======================================================\n",
    "\n",
    "In this notebook, we demonstrate how to take a lightcone file generated by the SC SAM (run within merger trees tailored to the CANDELS field geometries) and run it through our \"CANDELizing\" pipeline. The pipeline has several goals including correcting the mock catalog for CANDELS completeness limitations, corrected galaxy sizes, and adding photometric noise.\n",
    "\n",
    "Dependencies:\n",
    "* python (2.7+ probably best)\n",
    "* NumPy (http://www.numpy.org/)\n",
    "* Pandas (http://pandas.pydata.org/)\n",
    "* PyTables (http://www.pytables.org/)\n",
    "* CosmoloPy (http://roban.github.io/CosmoloPy/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download\n",
    "========================\n",
    "First, download the pipeline repository, if you haven't already, and move into that directory. It can be found at:\n",
    "\n",
    "https://github.com/yotamcohen/SAM-CANDELS_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what's inside:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CANDELizing_pipeline_tutorial.ipynb\n",
      "example_lightcone.dat\n",
      "lib/\n",
      "python_scripts/\n",
      "readme.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls -F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so we see directories containing library files and python scripts, and an example lightcone file called `example_lightcone.dat`, which we will use in this demonstration. This example lightcone file is a subset of a full `lightcone.dat` file from a **GOODS-S** realization, trimmed down to a managable size for example purposes. You can also use it for testing the pipeline on your own machine. The lightcone file is expected to be in the particular format as the example one we have here, complete with the header rows and all of the columns output by the SAM code. The version of the SAM code used for this paper writes 142 columns to the `lightcone.dat` file by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 0 halo_id_nbody\n",
      "# 1 gal_id\n",
      "# 2 gal_type (0, 1, 2)\n",
      "# 3 z_nopec [] (distance redshift w/o pec velocity)\n",
      "# 4 redshift []\n",
      "# 5 ra [degree]\n",
      "# 6 dec [degree]\n",
      "# 7 m_vir [1.0E10 M_sun]\n",
      "# 8 V_vir [km/s]\n",
      "# 9 r_vir [Mpc]\n",
      "# 10 c_NFW []\n",
      "# 11 spin []\n",
      "# 12 mstar_diffuse [1.0E10 M_sun]\n",
      "# 13 m_hot_halo [1.0E10 M_sun]\n",
      "# 14 Z_hot_halo [Z_sun]\n",
      "# 15 v_disk [km/s]\n",
      "# 16 r_disk [kpc]\n",
      "# 17 sigma_bulge [km/s]\n",
      "# 18 rbulge [kpc]\n",
      "# 19 mhalo [1.0E10 M_sun]\n",
      "# 20 mstar [1.0E10 M_sun]\n",
      "# 21 mcold [1.0E10 M_sun]\n",
      "# 22 mbulge [1.0E10 M_sun]\n",
      "# 23 mbh [1.0E10 M_sun]\n",
      "# 24 maccdot [M_sun/yr]\n",
      "# 25 maccdot_radio [M_sun/yr]\n",
      "# 26 Zstar [Z_sun]\n",
      "# 27 Zcold [Z_sun]\n",
      "# 28 mstardot [M_sun/yr]\n",
      "# 29 sfr_ave d [M_sun/yr]\n",
      "# 30 meanage [Gyr]\n",
      "# 31 tmerge [Gyr]\n",
      "# 32 tmajmerge [Gyr]\n",
      "# 33 cosi []\n",
      "# 34 UV1500_rest CANDELS/TOPHAT/UV1500top.dat\n",
      "# 35 UV1500_rest_bulge CANDELS/TOPHAT/UV1500top.dat\n",
      "# 36 UV1500_rest_dust CANDELS/TOPHAT/UV1500top.dat\n",
      "# 37 UV2300_rest CANDELS/TOPHAT/UV2300top.dat\n",
      "# 38 UV2300_rest_bulge CANDELS/TOPHAT/UV2300top.dat\n",
      "# 39 UV2300_rest_dust CANDELS/TOPHAT/UV2300top.dat\n",
      "# 40 UV2800_rest CANDELS/TOPHAT/UV2800top.dat\n",
      "# 41 UV2800_rest_bulge CANDELS/TOPHAT/UV2800top.dat\n",
      "# 42 UV2800_rest_dust CANDELS/TOPHAT/UV2800top.dat\n",
      "# 43 U_rest CANDELS/Johnson/Johnson_U.dat\n",
      "# 44 U_rest_bulge CANDELS/Johnson/Johnson_U.dat\n",
      "# 45 U_rest_dust CANDELS/Johnson/Johnson_U.dat\n",
      "# 46 B_rest CANDELS/Johnson/Johnson_B.dat\n",
      "# 47 B_rest_bulge CANDELS/Johnson/Johnson_B.dat\n",
      "# 48 B_rest_dust CANDELS/Johnson/Johnson_B.dat\n",
      "# 49 V_rest CANDELS/Johnson/Johnson_V.dat\n",
      "# 50 V_rest_bulge CANDELS/Johnson/Johnson_V.dat\n",
      "# 51 V_rest_dust CANDELS/Johnson/Johnson_V.dat\n",
      "# 52 R_rest CANDELS/Johnson/Cousins_R.dat\n",
      "# 53 R_rest_bulge CANDELS/Johnson/Cousins_R.dat\n",
      "# 54 R_rest_dust CANDELS/Johnson/Cousins_R.dat\n",
      "# 55 I_rest CANDELS/Johnson/Cousins_I.dat\n",
      "# 56 I_rest_bulge CANDELS/Johnson/Cousins_I.dat\n",
      "# 57 I_rest_dust CANDELS/Johnson/Cousins_I.dat\n",
      "# 58 J_rest CANDELS/Johnson/Bessell_J.dat\n",
      "# 59 J_rest_bulge CANDELS/Johnson/Bessell_J.dat\n",
      "# 60 J_rest_dust CANDELS/Johnson/Bessell_J.dat\n",
      "# 61 H_rest CANDELS/Johnson/Bessell_H.dat\n",
      "# 62 H_rest_bulge CANDELS/Johnson/Bessell_H.dat\n",
      "# 63 H_rest_dust CANDELS/Johnson/Bessell_H.dat\n",
      "# 64 K_rest CANDELS/Johnson/Bessell_K.dat\n",
      "# 65 K_rest_bulge CANDELS/Johnson/Bessell_K.dat\n",
      "# 66 K_rest_dust CANDELS/Johnson/Bessell_K.dat\n",
      "# 67 galex_FUV CANDELS/GALEX/galex_FUV.dat\n",
      "# 68 galex_FUV_bulge CANDELS/GALEX/galex_FUV.dat\n",
      "# 69 galex_FUV_dust CANDELS/GALEX/galex_FUV.dat\n",
      "# 70 galex_NUV CANDELS/GALEX/galex_NUV.dat\n",
      "# 71 galex_NUV_bulge CANDELS/GALEX/galex_NUV.dat\n",
      "# 72 galex_NUV_dust CANDELS/GALEX/galex_NUV.dat\n",
      "# 73 sdss_u CANDELS/SDSS/sdss_u.dat\n",
      "# 74 sdss_u_bulge CANDELS/SDSS/sdss_u.dat\n",
      "# 75 sdss_u_dust CANDELS/SDSS/sdss_u.dat\n",
      "# 76 sdss_g CANDELS/SDSS/sdss_g.dat\n",
      "# 77 sdss_g_bulge CANDELS/SDSS/sdss_g.dat\n",
      "# 78 sdss_g_dust CANDELS/SDSS/sdss_g.dat\n",
      "# 79 sdss_r CANDELS/SDSS/sdss_r.dat\n",
      "# 80 sdss_r_bulge CANDELS/SDSS/sdss_r.dat\n",
      "# 81 sdss_r_dust CANDELS/SDSS/sdss_r.dat\n",
      "# 82 sdss_i CANDELS/SDSS/sdss_i.dat\n",
      "# 83 sdss_i_bulge CANDELS/SDSS/sdss_i.dat\n",
      "# 84 sdss_i_dust CANDELS/SDSS/sdss_i.dat\n",
      "# 85 sdss_z CANDELS/SDSS/sdss_z.dat\n",
      "# 86 sdss_z_bulge CANDELS/SDSS/sdss_z.dat\n",
      "# 87 sdss_z_dust CANDELS/SDSS/sdss_z.dat\n",
      "# 88 acsf435w CANDELS/ACS/f435w.WFC1.dat\n",
      "# 89 acsf435w_bulge CANDELS/ACS/f435w.WFC1.dat\n",
      "# 90 acsf435w_dust CANDELS/ACS/f435w.WFC1.dat\n",
      "# 91 acsf606w CANDELS/ACS/f606w.WFC1.dat\n",
      "# 92 acsf606w_bulge CANDELS/ACS/f606w.WFC1.dat\n",
      "# 93 acsf606w_dust CANDELS/ACS/f606w.WFC1.dat\n",
      "# 94 acsf775w CANDELS/ACS/f775w.WFC1.dat\n",
      "# 95 acsf775w_bulge CANDELS/ACS/f775w.WFC1.dat\n",
      "# 96 acsf775w_dust CANDELS/ACS/f775w.WFC1.dat\n",
      "# 97 acsf814w CANDELS/ACS/f814w.WFC1.dat\n",
      "# 98 acsf814w_bulge CANDELS/ACS/f814w.WFC1.dat\n",
      "# 99 acsf814w_dust CANDELS/ACS/f814w.WFC1.dat\n",
      "# 100 acsf850lp CANDELS/ACS/f850lp.WFC1.dat\n",
      "# 101 acsf850lp_bulge CANDELS/ACS/f850lp.WFC1.dat\n",
      "# 102 acsf850lp_dust CANDELS/ACS/f850lp.WFC1.dat\n",
      "# 103 wfc3f275w CANDELS/WFC3/f275w.UVIS1.dat\n",
      "# 104 wfc3f275w_bulge CANDELS/WFC3/f275w.UVIS1.dat\n",
      "# 105 wfc3f275w_dust CANDELS/WFC3/f275w.UVIS1.dat\n",
      "# 106 wfc3f336w CANDELS/WFC3/f336w.UVIS1.dat\n",
      "# 107 wfc3f336w_bulge CANDELS/WFC3/f336w.UVIS1.dat\n",
      "# 108 wfc3f336w_dust CANDELS/WFC3/f336w.UVIS1.dat\n",
      "# 109 wfc3f105w CANDELS/WFC3/f105w.IR.dat\n",
      "# 110 wfc3f105w_bulge CANDELS/WFC3/f105w.IR.dat\n",
      "# 111 wfc3f105w_dust CANDELS/WFC3/f105w.IR.dat\n",
      "# 112 wfc3f125w CANDELS/WFC3/f125w.IR.dat\n",
      "# 113 wfc3f125w_bulge CANDELS/WFC3/f125w.IR.dat\n",
      "# 114 wfc3f125w_dust CANDELS/WFC3/f125w.IR.dat\n",
      "# 115 wfc3f160w CANDELS/WFC3/f160w.IR.dat\n",
      "# 116 wfc3f160w_bulge CANDELS/WFC3/f160w.IR.dat\n",
      "# 117 wfc3f160w_dust CANDELS/WFC3/f160w.IR.dat\n",
      "# 118 ctio_U CANDELS/CTIO/U_ctio_mosaic_tot.dat\n",
      "# 119 ctio_U_bulge CANDELS/CTIO/U_ctio_mosaic_tot.dat\n",
      "# 120 ctio_U_dust CANDELS/CTIO/U_ctio_mosaic_tot.dat\n",
      "# 121 CFHTLS_u CANDELS/CFHTLS/uMega.fil\n",
      "# 122 CFHTLS_u_bulge CANDELS/CFHTLS/uMega.fil\n",
      "# 123 CFHTLS_u_dust CANDELS/CFHTLS/uMega.fil\n",
      "# 124 musyc_u38 CANDELS/MUSYC/ecdfs.U38.filt.dat\n",
      "# 125 musyc_u38_bulge CANDELS/MUSYC/ecdfs.U38.filt.dat\n",
      "# 126 musyc_u38_dust CANDELS/MUSYC/ecdfs.U38.filt.dat\n",
      "# 127 UKIRT_J CANDELS/UKIRT/J_filter.dat\n",
      "# 128 UKIRT_J_bulge CANDELS/UKIRT/J_filter.dat\n",
      "# 129 UKIRT_J_dust CANDELS/UKIRT/J_filter.dat\n",
      "# 130 UKIRT_H CANDELS/UKIRT/H_filter.dat\n",
      "# 131 UKIRT_H_bulge CANDELS/UKIRT/H_filter.dat\n",
      "# 132 UKIRT_H_dust CANDELS/UKIRT/H_filter.dat\n",
      "# 133 UKIRT_K CANDELS/UKIRT/K_filter.dat\n",
      "# 134 UKIRT_K_bulge CANDELS/UKIRT/K_filter.dat\n",
      "# 135 UKIRT_K_dust CANDELS/UKIRT/K_filter.dat\n",
      "# 136 irac_ch1 CANDELS/IRAC/irac_ch1.dat\n",
      "# 137 irac_ch1_bulge CANDELS/IRAC/irac_ch1.dat\n",
      "# 138 irac_ch1_dust CANDELS/IRAC/irac_ch1.dat\n",
      "# 139 irac_ch2 CANDELS/IRAC/irac_ch2.dat\n",
      "# 140 irac_ch2_bulge CANDELS/IRAC/irac_ch2.dat\n",
      "# 141 irac_ch2_dust CANDELS/IRAC/irac_ch2.dat\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head -n 142 example_lightcone.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so now lets get to the pipeline..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: ascii text file to hdf5 file\n",
    "========\n",
    "\n",
    "The native format of the `lightcone.dat` files output by the SAM is simple ASCII text, which can be very slow to read/write with python, so the first step is to store all the data in an hdf5 file. For this, we will use `python_scripts/rawlightconefile_to_h5.py` which will do just that. This script is expecting a file header file containing the header rows to live in the `lib/` directory; if it doesn't already exist, you can just do something like\n",
    "\n",
    "`head -n 142 example_lightcone.dat > lib/headerfile.txt`\n",
    "\n",
    "If running from the command line, the usage for `rawlightconefile_to_h5.py` is as follows:\n",
    "\n",
    "~~~\n",
    "python python_scripts/rawlightconefile_to_h5.py <headerfile> <lightcone file> <desired path to h5file>\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grabbed lightcone of shape (12158, 142)\n",
      "writing to h5 file...\n",
      "saved lightcone data to ./example_lightcone.h5\n",
      "all units are same as in original lightcone data file\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head -n 142 example_lightcone.dat > lib/headerfile.txt\n",
    "python python_scripts/rawlightconefile_to_h5.py lib/headerfile.txt ./example_lightcone.dat ./example_lightcone.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 61048\n",
      "-rw-r--r--   1 yotam  staff    36B Oct 19 18:26 readme.txt\n",
      "drwxr-xr-x  10 yotam  staff   340B Oct 19 18:26 python_scripts/\n",
      "-rw-r--r--   1 yotam  staff    16M Oct 19 18:26 example_lightcone.dat\n",
      "-rw-r--r--   1 yotam  staff    35K Oct 19 18:26 CANDELizing_pipeline_tutorial.ipynb\n",
      "drwxr-xr-x  23 yotam  staff   782B Oct 19 18:27 lib/\n",
      "-rw-r--r--   1 yotam  staff    13M Oct 19 18:28 example_lightcone.h5\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls -ltrhF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so we see the file was created. Lets open it up to make sure everythings there. You can easily open it with pandas in an interactive python sessios (like this notebook). By default, the data will be stored under the `'dat'` key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   halo_id_nbody  gal_id  gal_type   z_nopec  redshift         ra        dec  \\\n",
      "0     2776575133       1         0  0.018547  0.020271  53.374077 -27.527531   \n",
      "1     2691101762       1         0  0.074530  0.076570  52.987942 -27.819973   \n",
      "2     2677376520      12         2  0.075058  0.077036  53.005903 -27.613504   \n",
      "3     2661838289      16         2  0.085732  0.083417  53.351725 -27.662076   \n",
      "4     2661851549       1         0  0.090251  0.089146  52.952389 -27.695505   \n",
      "\n",
      "       m_vir      V_vir     r_vir      ...        UKIRT_H_dust    UKIRT_K  \\\n",
      "0   2.423304  37.088756  0.075312      ...           20.788730  21.144127   \n",
      "1   1.485251  32.040132  0.061878      ...           25.996959  26.278141   \n",
      "2  14.719764  68.831694  0.132876      ...           21.741104  21.965658   \n",
      "3  10.148968  60.924214  0.116946      ...           23.278740  23.523828   \n",
      "4   1.074189  28.868147  0.055132      ...           26.129287  26.398970   \n",
      "\n",
      "   UKIRT_K_bulge  UKIRT_K_dust   irac_ch1  irac_ch1_bulge  irac_ch1_dust  \\\n",
      "0      24.537762     21.144127  21.939049       25.337122      21.939049   \n",
      "1      29.281053     26.278141  27.071520       30.078001      27.071520   \n",
      "2      23.674726     21.970118  22.752666       24.458108      22.752746   \n",
      "3      27.004021     23.523828  24.317908       27.795882      24.317908   \n",
      "4      29.779144     26.398970  27.193208       30.574421      27.193208   \n",
      "\n",
      "    irac_ch2  irac_ch2_bulge  irac_ch2_dust  \n",
      "0  22.395286       25.802917      22.395286  \n",
      "1  27.457822       30.507695      27.457822  \n",
      "2  23.196188       24.908618      23.196188  \n",
      "3  24.729002       28.237642      24.729002  \n",
      "4  27.582015       31.002708      27.582015  \n",
      "\n",
      "[5 rows x 142 columns]\n"
     ]
    }
   ],
   "source": [
    "store = HDFStore('example_lightcone.h5')\n",
    "lcone = store['dat']\n",
    "store.close()\n",
    "\n",
    "print lcone.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there's our lightcone data, now stored in h5 format for efficient reading/writing for the rest of the pipeline and just accessing the data in general for e.g. plotting, analysis, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: galaxy size corrections\n",
    "==================\n",
    "\n",
    "The SAM code reports the 3D bulge and disk sizes. We want to also have the 2D (projected) sizes. There is a perscription for how to do this conversion, which is implemented in `python_scripts/galaxy_size_corrections.py`. The usage is simple, just call the script with the h5file from the previous step as the only argument:\n",
    "\n",
    "~~~\n",
    "python python_scripts/galaxy_size_corrections.py <lightcone h5 file>\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grabbed dataframe\n",
      "dataframe now includes projected disk size\n",
      "dataframe now includes projected bulge size\n",
      "dataframe now includes bulge-to-total ratio (based on light)\n",
      "dataframe now includes bulge-to-total ratio (based on mass)\n",
      "saved dataframe to example_lightcone.h5\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python python_scripts/galaxy_size_corrections.py example_lightcone.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the output..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   halo_id_nbody  gal_id  gal_type   z_nopec  redshift         ra        dec  \\\n",
      "0     2776575133       1         0  0.018547  0.020271  53.374077 -27.527531   \n",
      "1     2691101762       1         0  0.074530  0.076570  52.987942 -27.819973   \n",
      "2     2677376520      12         2  0.075058  0.077036  53.005903 -27.613504   \n",
      "3     2661838289      16         2  0.085732  0.083417  53.351725 -27.662076   \n",
      "4     2661851549       1         0  0.090251  0.089146  52.952389 -27.695505   \n",
      "\n",
      "       m_vir      V_vir     r_vir    ...      irac_ch1  irac_ch1_bulge  \\\n",
      "0   2.423304  37.088756  0.075312    ...     21.939049       25.337122   \n",
      "1   1.485251  32.040132  0.061878    ...     27.071520       30.078001   \n",
      "2  14.719764  68.831694  0.132876    ...     22.752666       24.458108   \n",
      "3  10.148968  60.924214  0.116946    ...     24.317908       27.795882   \n",
      "4   1.074189  28.868147  0.055132    ...     27.193208       30.574421   \n",
      "\n",
      "   irac_ch1_dust   irac_ch2  irac_ch2_bulge  irac_ch2_dust  r_disk_proj_H  \\\n",
      "0      21.939049  22.395286       25.802917      22.395286       4.818587   \n",
      "1      27.071520  27.457822       30.507695      27.457822       7.511574   \n",
      "2      22.752746  23.196188       24.908618      23.196188       2.644919   \n",
      "3      24.317908  24.729002       28.237642      24.729002       5.815210   \n",
      "4      27.193208  27.582015       31.002708      27.582015       3.264203   \n",
      "\n",
      "   r_bulge_proj_H  BT_light   BT_mass  \n",
      "0        0.002155  0.044357  0.062245  \n",
      "1        0.000002  0.062672  0.120755  \n",
      "2        0.102814  0.208211  0.244035  \n",
      "3        0.004822  0.041531  0.072405  \n",
      "4        0.000007  0.043981  0.087022  \n",
      "\n",
      "[5 rows x 146 columns]\n"
     ]
    }
   ],
   "source": [
    "store = HDFStore('example_lightcone.h5')\n",
    "lcone = store['dat']\n",
    "store.close()\n",
    "\n",
    "print lcone.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are a few new columns at the end of the dataframe, including the projected bulge and disk sizes (in the original units of kpc) and bulge/total ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: various other parameters\n",
    "===========\n",
    "\n",
    "One of the ultimate goals of this pipeline is to do a completeness correction on the mock catalogs given the completeness limits of CANDELS. People at STScI have generated lookup tables which give the probability of detecting a galaxy given it's H-band magnitude, effective radius in pixels (given the plate scale of HSTWFC3F160W), and sersic index. In order to use these, we need to calculate and add these parameters to our mock galaxy catalog.\n",
    "\n",
    "* The effective radius and sersic index calculation is done using a model which takes as input the bulge/total ratio (based on light) and bulge size to disk size ratio.\n",
    "* In order to calculate the size in pixels, we first need the angular size. The galaxies are far away, so the small angle approximation tells us that the angular size is just given by the physical size divided by the distance. Since our mock galaxies have large, cosmological redshifts, the distance must be calculated using a cosmology library (this is where CosmoloPy comes in). Once we know the distance, we calculate the angular size, and then we calculate the size in pixels simply by knowning the plate scale of HSTWFC3.\n",
    "\n",
    "The above is implemented in `python_scripts/galaxy_profiles.py`. Usage is simple:\n",
    "\n",
    "~~~\n",
    "python python_scripts/galaxy_profiles.py <lighcone h5 file>\n",
    "~~~\n",
    "\n",
    "(Beware that the sersic index calculation has large time complexity so it could take several minutes or even more than an hour for a full (>1Gb) lightcone.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grabbed dataframe from example_lightcone.h5\n",
      "dataframe now includes bulge/disk ratio from projected sizes\n",
      "dataframe now includes sersic profile parameters\n",
      "dataframe now includes galaxy radii in PIXELS as seen by HSTWFC3F160W\n",
      "saved dataframe to example_lightcone.h5\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python python_scripts/galaxy_profiles.py example_lightcone.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, lets check the output..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   halo_id_nbody  gal_id  gal_type   z_nopec  redshift         ra        dec  \\\n",
      "0     2776575133       1         0  0.018547  0.020271  53.374077 -27.527531   \n",
      "1     2691101762       1         0  0.074530  0.076570  52.987942 -27.819973   \n",
      "2     2677376520      12         2  0.075058  0.077036  53.005903 -27.613504   \n",
      "3     2661838289      16         2  0.085732  0.083417  53.351725 -27.662076   \n",
      "4     2661851549       1         0  0.090251  0.089146  52.952389 -27.695505   \n",
      "\n",
      "       m_vir      V_vir     r_vir    ...      r_bulge_proj_H  BT_light  \\\n",
      "0   2.423304  37.088756  0.075312    ...            0.002155  0.044357   \n",
      "1   1.485251  32.040132  0.061878    ...            0.000002  0.062672   \n",
      "2  14.719764  68.831694  0.132876    ...            0.102814  0.208211   \n",
      "3  10.148968  60.924214  0.116946    ...            0.004822  0.041531   \n",
      "4   1.074189  28.868147  0.055132    ...            0.000007  0.043981   \n",
      "\n",
      "    BT_mass       RBD  n_sersic      reff            d_a            d_L  \\\n",
      "0  0.062245  0.000447  1.098090  4.642019   87438.757370   91019.629232   \n",
      "1  0.120755  0.000000  1.143273  7.107273  308849.047842  357956.962155   \n",
      "2  0.244035  0.038872  1.700575  2.102467  310559.280446  360250.798019   \n",
      "3  0.072405  0.000829  1.091413  5.617344  333786.931111  391796.561585   \n",
      "4  0.087022  0.000002  1.097198  3.145729  354341.651839  420333.890037   \n",
      "\n",
      "   angular_radius   r_pixels  \n",
      "0       10.950352  84.233479  \n",
      "1        4.746592  36.512243  \n",
      "2        1.396400  10.741537  \n",
      "3        3.471257  26.701980  \n",
      "4        1.831152  14.085781  \n",
      "\n",
      "[5 rows x 153 columns]\n"
     ]
    }
   ],
   "source": [
    "store = HDFStore('example_lightcone.h5')\n",
    "lcone = store['dat']\n",
    "store.close()\n",
    "\n",
    "print lcone.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are several new columns. Just to be clear, lets name them all here:\n",
    "* `RBD` : ratio of projected bulge radius to projected disk radius\n",
    "* `n_sersic` : sersic index\n",
    "* `reff`: effective radius in kpc\n",
    "* `d_a` : cosmological angular diameter distance in kpc\n",
    "* `d_L` : cosmological luminosity distance in kpc\n",
    "* `angular_radius` : in arcseconds\n",
    "* `r_pixels` : given the plate scale of HSTWFC3F160W of 0.13 arcsec/pixel (correct me if I'm wrong on that value...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: add photometric noise\n",
    "===============\n",
    "\n",
    "We would now like to add noise to our mock galaxies photometry. One of the main reasons for doing this is because in the following step, we would like to estimate the detection probability at a given observed magnitude. Real observations in the survey are plagued by noise. Thus, we would like to replicate the effect of this noise, and then use the noisy photometry (as opposed to the clean photometry) as the input to our detection probability calculation. The systematics of HST and noise background in various bandpasses is well-understood, so we use this information in order to assign photometric noise to the mock photometry. This is accomplished by running `python_scripts/photometric_noise.py`. Usage is simple:\n",
    "\n",
    "~~~\n",
    "python python_scripts/photometric_noise.py <lightcone h5 file>\n",
    "~~~\n",
    "\n",
    "You may consider this script to be a black box, or you can read the code if you are curious, but beware that it is messy. Anyway, let's try it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grabbed lightcone\n",
      "grabbed photometric data from CANDELS catalog\n",
      "adding noise to acsf435w\n",
      "adding noise to acsf606w\n",
      "adding noise to acsf775w\n",
      "adding noise to acsf814w\n",
      "adding noise to acsf850lp\n",
      "adding noise to wfc3f275w\n",
      "adding noise to wfc3f105w\n",
      "adding noise to wfc3f125w\n",
      "adding noise to wfc3f160w\n",
      "adding noise to ctio_U\n",
      "adding noise to CFHTLS_u\n",
      "adding noise to UKIRT_J\n",
      "adding noise to UKIRT_H\n",
      "adding noise to UKIRT_K\n",
      "adding noise to irac_ch1\n",
      "adding noise to irac_ch2\n",
      "dataframe now includes photometric noise\n",
      "saved to example_lightcone.h5\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python python_scripts/photometric_noise.py example_lightcone.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what it did to our dataframe..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   halo_id_nbody  gal_id  gal_type   z_nopec  redshift         ra        dec  \\\n",
      "0     2776575133       1         0  0.018547  0.020271  53.374077 -27.527531   \n",
      "1     2691101762       1         0  0.074530  0.076570  52.987942 -27.819973   \n",
      "2     2677376520      12         2  0.075058  0.077036  53.005903 -27.613504   \n",
      "3     2661838289      16         2  0.085732  0.083417  53.351725 -27.662076   \n",
      "4     2661851549       1         0  0.090251  0.089146  52.952389 -27.695505   \n",
      "\n",
      "       m_vir      V_vir     r_vir         ...           UKIRT_J_noisy  \\\n",
      "0   2.423304  37.088756  0.075312         ...               20.825953   \n",
      "1   1.485251  32.040132  0.061878         ...               26.247231   \n",
      "2  14.719764  68.831694  0.132876         ...               21.809387   \n",
      "3  10.148968  60.924214  0.116946         ...               23.326041   \n",
      "4   1.074189  28.868147  0.055132         ...               26.416156   \n",
      "\n",
      "   UKIRT_J_dust_noisy  UKIRT_H_noisy  UKIRT_H_dust_noisy  UKIRT_K_noisy  \\\n",
      "0           20.825869      20.798973           20.798717      21.151222   \n",
      "1           26.275786      26.681945           26.681735      26.789901   \n",
      "2           21.823665      21.752080           21.761830      21.980837   \n",
      "3           23.331092      23.355174           23.351986      23.574703   \n",
      "4           26.455734      26.890374           26.968886      27.002930   \n",
      "\n",
      "   UKIRT_K_dust_noisy  irac_ch1_noisy  irac_ch1_dust_noisy  irac_ch2_noisy  \\\n",
      "0           21.151768       21.961374            21.963020       22.427212   \n",
      "1           26.788844       29.448826            30.605252       31.053016   \n",
      "2           21.984807       22.818373            22.814096       23.284849   \n",
      "3           23.575227       24.537386            24.522573       25.029469   \n",
      "4           26.950074       30.220578            29.788150       31.840870   \n",
      "\n",
      "   irac_ch2_dust_noisy  \n",
      "0            22.443462  \n",
      "1            30.862398  \n",
      "2            23.265170  \n",
      "3            25.023583  \n",
      "4            31.719088  \n",
      "\n",
      "[5 rows x 185 columns]\n"
     ]
    }
   ],
   "source": [
    "store = HDFStore('example_lightcone.h5')\n",
    "lcone = store['dat']\n",
    "store.close()\n",
    "\n",
    "print lcone.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, for each filterband, we have two new columns, an estimate of the noisy photometry with and without dust. At last, we are ready to make our detection probability determinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: detection probability estimation\n",
    "============\n",
    "\n",
    "We now have all the information needed to lookup the probability of detecting a given mock galaxy given the limitations of CANDELS and the properties of the galaxy. This is implemented in `python_scripts/calculate_dp.py`. Usage is as follows:\n",
    "\n",
    "~~~\n",
    "python python_scripts/calculate_dp.py <lightcone h5 file> <table for disks> <table for ellipticals>\n",
    "~~~\n",
    "\n",
    "The lookup tables are in `lib/completeness_tables`. There are tables for each CANDELS field, and they are further split by the galaxy type. All you have to do is call the script with the lookup table files for the CANDELS field corresponding to your lightcone file. In this example, we have been using a subset of a **GOODS-S** lightcone file, so we will call those lookup tables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on example_lightcone.h5\n",
      "dataframe now includes detection probabilities\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python python_scripts/calculate_dp.py example_lightcone.h5 \\\n",
    "./lib/completeness_tables/goodss_expdisk.npz \\\n",
    "./lib/completeness_tables/goodss_devauc.npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check it out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   halo_id_nbody  gal_id  gal_type   z_nopec  redshift         ra        dec  \\\n",
      "0     2776575133       1         0  0.018547  0.020271  53.374077 -27.527531   \n",
      "1     2691101762       1         0  0.074530  0.076570  52.987942 -27.819973   \n",
      "2     2677376520      12         2  0.075058  0.077036  53.005903 -27.613504   \n",
      "3     2661838289      16         2  0.085732  0.083417  53.351725 -27.662076   \n",
      "4     2661851549       1         0  0.090251  0.089146  52.952389 -27.695505   \n",
      "\n",
      "       m_vir      V_vir     r_vir    ...     UKIRT_J_dust_noisy  \\\n",
      "0   2.423304  37.088756  0.075312    ...              20.825869   \n",
      "1   1.485251  32.040132  0.061878    ...              26.275786   \n",
      "2  14.719764  68.831694  0.132876    ...              21.823665   \n",
      "3  10.148968  60.924214  0.116946    ...              23.331092   \n",
      "4   1.074189  28.868147  0.055132    ...              26.455734   \n",
      "\n",
      "   UKIRT_H_noisy  UKIRT_H_dust_noisy  UKIRT_K_noisy  UKIRT_K_dust_noisy  \\\n",
      "0      20.798973           20.798717      21.151222           21.151768   \n",
      "1      26.681945           26.681735      26.789901           26.788844   \n",
      "2      21.752080           21.761830      21.980837           21.984807   \n",
      "3      23.355174           23.351986      23.574703           23.575227   \n",
      "4      26.890374           26.968886      27.002930           26.950074   \n",
      "\n",
      "   irac_ch1_noisy  irac_ch1_dust_noisy  irac_ch2_noisy  irac_ch2_dust_noisy  \\\n",
      "0       21.961374            21.963020       22.427212            22.443462   \n",
      "1       29.448826            30.605252       31.053016            30.862398   \n",
      "2       22.818373            22.814096       23.284849            23.265170   \n",
      "3       24.537386            24.522573       25.029469            25.023583   \n",
      "4       30.220578            29.788150       31.840870            31.719088   \n",
      "\n",
      "         dp  \n",
      "0  1.000000  \n",
      "1  0.000000  \n",
      "2  0.920778  \n",
      "3  0.670948  \n",
      "4  0.000000  \n",
      "\n",
      "[5 rows x 186 columns]\n"
     ]
    }
   ],
   "source": [
    "store = HDFStore('example_lightcone.h5')\n",
    "lcone = store['dat']\n",
    "store.close()\n",
    "\n",
    "print lcone.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "There is one new column, called dp (detection probability). Now we can do whatever we want with this, like running a detection scheme (e.g. `python_scripts/detection_scheme.py`). Let's just look at some diagnostics to make sure things make sense: (coming soon, but they do make sense and the pipeline is a success!)"
=======
    "There is one new column, called dp (detection probability). Now we can do whatever we want with this, like running a detection scheme (e.g. `python_scripts/detection_scheme.py`)."
>>>>>>> d2d2ff6befe518efc218f77dd06fc95e7008ae16
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
